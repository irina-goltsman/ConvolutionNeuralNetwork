==========polarity=======================
dataset name: ./prepocessed_data/polarity_mr_100
classifier name: MultinomialNB
best params:
{'clf__fit_prior': False, 'clf__alpha': 0.70000000000000007}
best score = 0.792534

dataset name: ./prepocessed_data/polarity_mr_100
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'hinge', 'clf__fit_intercept': True, 'clf__alpha': 7.0000000000000007e-05}
best score = 0.785312

dataset name: ./prepocessed_data/polarity_mr_100
classifier name: LogisticRegression
best params:
{'clf__C': 1.8999999999999997, 'clf__fit_intercept': False, 'clf__solver': 'newton-cg'}
best score = 0.769931

dataset name: ./prepocessed_data/polarity_mr_100
adam
Train score = 0.993990. Valid score = 0.786667.
OPTIMIZATION COMPLETE.
Best valid score: 0.813333
Best iter num: 200, best epoch: 2

==========mr_kaggle=======================
dcnn?? / 1cnn, mr_kaggle
global_iter 2380, epoch 6, batch 130, mean train cost = 0.187806
------------valid score: 0.893200------------

SGDClassifier, mr_kaggle
clf__penalty=l2, clf__loss=hinge, clf__fit_intercept=False, clf__alpha=1e-05, score=0.905200 -  24.5s

LogisticRegression чуть похуже

dataset name: ./prepocessed_data/mr_kaggle_mr_100
classifier name: MultinomialNB
best params:
{'clf__fit_prior': True, 'clf__alpha': 0.70000000000000007}
best score = 0.890040

==========20_news=============
dataset name: ./prepocessed_data/20_news
classifier name: MultinomialNB
best params:
{'clf__fit_prior': False, 'clf__alpha': 0.10000000000000001}
best score = 0.732039

20_news
clf__penalty=elasticnet, clf__loss=hinge, clf__fit_intercept=True, clf__alpha=2e-05, score=0.7692

dataset name: ../../hdfs/preprocessed_data/20_news
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'squared_loss', 'clf__fit_intercept': True, 'clf__alpha': 4.0000000000000003e-05}
best score = 0.772259


==========twitter=============
dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: MultinomialNB
        'clf__alpha': np.arange(0.1, 2.0, 0.3),
        'clf__fit_prior': (True, False)
best params:
{'clf__fit_prior': True, 'clf__alpha': 1.0000000000000002}
best score = 0.788536

dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'modified_huber', 'clf__fit_intercept': True, 'clf__alpha': 1.0000000000000001e-05}
best score = 0.811756
--- 2784.93501496 seconds ---

dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: LogisticRegression
best params:
{'clf__penalty': 'l2', 'clf__C': 2.2000000000000002, 'clf__solver': 'liblinear'}
best score = 0.813469
--- 1923.76216292 seconds ---

tweets 200000 1cnn
Epoch 1 finished. Training time: 978.14 secs
Train score = 0.808994. Valid score = 0.756800.

==========bin_sent=============
clf_name = dcnn
vocab_size = 15449
word_dimension = 48
non_static = True
sentence_len = 58
n_out = 2
windows = ((7,), (5,))
n_filters = (6, 14)
n_hidden = 10
k_top = 4
activation = ('tanh', 'tanh')
dropout = 0.500000
l1_regs = ()
l2_regs = (5e-05, 1.5e-05, 1.5e-06, 5e-05)
batch_size = 40
seed = 0
valid_frequency=10
update_finction=adadelta

global_iter 48020, epoch 12, batch 3063, mean train cost = 0.485674
------------valid score: 0.829128------------
Test score = 0.819879.

global_iter 49650, epoch 13, batch 606, mean train cost = 0.528234
------------valid score: 0.831422------------
Test score = 0.800659.

Достигла нормальных результатов только к 4ой эпохе

1cnn - 2
test_on_binary_sentiment(data_path='./data/binarySentiment/', clf_name='1cnn',
                         n_epochs=100, batch_size=50, non_static=True, early_stop=False, valid_frequency=20,
                         k_top=1, n_filters=(200,), windows=((3, 4),), seed=0,
                         word_dimentions=30, activations=('tanh',), dropout=0.2,
                         l1_regs=(0.00001, 0.00003, 0.000003, 0.0001),
                         update_finction=adadelta)
1cnn - 3
test_on_binary_sentiment(data_path='./data/binarySentiment/', clf_name='1cnn',
                         n_epochs=15, batch_size=50, non_static=True, early_stop=False, valid_frequency=20,
                         k_top=1, n_filters=(100,), windows=((3, 4),), seed=0,
                         word_dimentions=40, activations=('iden',), dropout=0.2,
                         l1_regs=(0.0001, 0.0003, 0.00003, 0.0001),
                         update_finction=adam)
1cnn - 3:
Epoch 15 finished. Training time: 482.64 secs
Train score = 0.814012. Valid score = 0.815367.
Test score = 0.809995
OPTIMIZATION COMPLETE.
Best valid score: 0.823394
Best iter num: 45620, best epoch: 14
Last test score = 0.811093
Saving state to './cnn_states/state_2016-06-20-12:35:33'...
--- 6383.1793108 seconds ---

==================================
