==========polarity=======================
dataset name: ./prepocessed_data/polarity_mr_100
classifier name: MultinomialNB
best params:
{'clf__fit_prior': False, 'clf__alpha': 0.70000000000000007}
best score = 0.792534

dataset name: ./prepocessed_data/polarity_mr_100
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'hinge', 'clf__fit_intercept': True, 'clf__alpha': 7.0000000000000007e-05}
best score = 0.785312

dataset name: ./prepocessed_data/polarity_mr_100
classifier name: LogisticRegression
best params:
{'clf__C': 1.8999999999999997, 'clf__fit_intercept': False, 'clf__solver': 'newton-cg'}
best score = 0.769931

dataset name: ./prepocessed_data/polarity_mr_100
adam
Train score = 0.993990. Valid score = 0.786667.
OPTIMIZATION COMPLETE.
Best valid score: 0.813333
Best iter num: 200, best epoch: 2

==========mr_kaggle=======================
dcnn?? / 1cnn, mr_kaggle
global_iter 2380, epoch 6, batch 130, mean train cost = 0.187806
------------valid score: 0.893200------------

SGDClassifier, mr_kaggle
clf__penalty=l2, clf__loss=hinge, clf__fit_intercept=False, clf__alpha=1e-05, score=0.905200 -  24.5s

LogisticRegression чуть похуже

dataset name: ./prepocessed_data/mr_kaggle_mr_100
classifier name: MultinomialNB
best params:
{'clf__fit_prior': True, 'clf__alpha': 0.70000000000000007}
best score = 0.890040

train_and_save_model(clf_name='1cnn', data_file=dt.get_output_name(dataset_name, model_name, max_size),
                     n_epochs=15, batch_size=50, non_static=True, early_stop=True, valid_frequency=40,
                     k_top=1, n_filters=(100,), windows=((3, 4, 5),), seed=0, update_finction=adam,
                     word_dimentions=None, activations=('relu',), dropout=0.5,
                     l2_regs=(0, 0.00005, 0.00005, 0.00005, 0.00005),
                     l1_regs=(0, 0.0001, 0.0001, 0.0001, 0.0001), n_hidden=100,
                     big_dataset=True)
global_iter 860, epoch 2, batch 410, mean train cost = 0.392985
------------valid score: 0.892000------------

clf_name = dcnn
vocab_size = 82404
word_dimension = 40
non_static = True
sentence_len = 689
n_out = 2
windows = ((7,), (5,))
n_filters = (6, 14)
n_hidden = 100
k_top = 4
activation = ('tanh', 'tanh')
dropout = 0.500000
l1_regs = []
l2_regs = (1e-05, 3e-05, 3e-06, 0.0001)
batch_size = 20
seed = 0

global_iter 3660, epoch 4, batch 285, mean train cost = 0.087630
------------valid score: 0.884800------------



AWS
Saving state to './cnn_states/state_2016-06-23-20:54:00'...

lstm 100 + LR
Epoch 1 finished. Training time: 168.44 secs
Train score = 0.940800. Valid score = 0.832800.
Epoch 2 finished. Training time: 164.34 secs
Train score = 0.996089. Valid score = 0.857200.
Epoch 3 finished. Training time: 171.05 secs
Train score = 0.999778. Valid score = 0.855200.
Epoch 4 finished. Training time: 171.15 secs
Train score = 1.000000. Valid score = 0.859600.

global_iter 3720, epoch 9, batch 120, mean train cost = 0.000392
------------valid score: 0.862000------------

gru + LR
Epoch 1 finished. Training time: 136.58 secs
Train score = 0.939511. Valid score = 0.832400.


==========20_news=============
dataset name: ./prepocessed_data/20_news
classifier name: MultinomialNB
best params:
{'clf__fit_prior': False, 'clf__alpha': 0.10000000000000001}
best score = 0.732039

dataset name: ./preprocessed_data/20_news
classifier name: LogisticRegression
best params:
{'clf__C': 5.0, 'clf__solver': 'newton-cg'}
best score = 0.762442
--- 11038.2728481 seconds ---

20_news
clf__penalty=elasticnet, clf__loss=hinge, clf__fit_intercept=True, clf__alpha=2e-05, score=0.7692

dataset name: ../../hdfs/preprocessed_data/20_news
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'squared_loss', 'clf__fit_intercept': True, 'clf__alpha': 4.0000000000000003e-05}
best score = 0.772259

Отдельно train и test:
best params:
{'clf__penalty': 'l2', 'clf__loss': 'hinge', 'clf__alpha': 5.0000000000000002e-05}
best score = 0.776648
test score = 0.710967

dcnn
clf_name = dcnn
vocab_size = 140842
word_dimension = 40
non_static = True
sentence_len = 680
n_out = 20
windows = ((7,), (5,))
n_filters = (6, 14)
n_hidden = 100
k_top = 4
activation = ('tanh', 'tanh')
dropout = 0.500000
l1_regs = []
l2_regs = (1e-05, 3e-05, 3e-06, 0.0001)
batch_size = 20
seed = 0

global_iter 1740, epoch 3, batch 42, mean train cost = 1.026645
------------valid score: 0.554255------------
global_iter 2740, epoch 4, batch 193, mean train cost = 0.498715
------------valid score: 0.582447------------
global_iter 14840, epoch 18, batch 407, mean train cost = 0.163678
------------valid score: 0.593085------------


lstm, 100units:
global_iter 2360, epoch 7, batch 320, mean train cost = 0.377156
------------valid score: 0.503784------------

==========twitter=============
dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: MultinomialNB
        'clf__alpha': np.arange(0.1, 2.0, 0.3),
        'clf__fit_prior': (True, False)
best params:
{'clf__fit_prior': True, 'clf__alpha': 1.0000000000000002}
best score = 0.788536

dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: SGDClassifier
best params:
{'clf__penalty': 'l2', 'clf__loss': 'modified_huber', 'clf__fit_intercept': True, 'clf__alpha': 1.0000000000000001e-05}
best score = 0.811756
--- 2784.93501496 seconds ---

dataset name: ../../hdfs/preprocessed_data/twitter
classifier name: LogisticRegression
best params:
{'clf__penalty': 'l2', 'clf__C': 2.2000000000000002, 'clf__solver': 'liblinear'}
best score = 0.813469
--- 1923.76216292 seconds ---

tweets 200000 1cnn
Epoch 1 finished. Training time: 978.14 secs
Train score = 0.808994. Valid score = 0.756800.

lstm word_dim = 50

==========bin_sent=============
clf_name = dcnn
vocab_size = 15449
word_dimension = 48
non_static = True
sentence_len = 58
n_out = 2
windows = ((7,), (5,))
n_filters = (6, 14)
n_hidden = 10
k_top = 4
activation = ('tanh', 'tanh')
dropout = 0.500000
l1_regs = ()
l2_regs = (5e-05, 1.5e-05, 1.5e-06, 5e-05)
batch_size = 40
seed = 0
valid_frequency=10
update_finction=adadelta

global_iter 48020, epoch 12, batch 3063, mean train cost = 0.485674
------------valid score: 0.829128------------
Test score = 0.819879.

global_iter 49650, epoch 13, batch 606, mean train cost = 0.528234
------------valid score: 0.831422------------
Test score = 0.800659.

Достигла нормальных результатов только к 4ой эпохе

1cnn - 2
test_on_binary_sentiment(data_path='./data/binarySentiment/', clf_name='1cnn',
                         n_epochs=100, batch_size=50, non_static=True, early_stop=False, valid_frequency=20,
                         k_top=1, n_filters=(200,), windows=((3, 4),), seed=0,
                         word_dimentions=30, activations=('tanh',), dropout=0.2,
                         l1_regs=(0.00001, 0.00003, 0.000003, 0.0001),
                         update_finction=adadelta)
1cnn - 3
test_on_binary_sentiment(data_path='./data/binarySentiment/', clf_name='1cnn',
                         n_epochs=15, batch_size=50, non_static=True, early_stop=False, valid_frequency=20,
                         k_top=1, n_filters=(100,), windows=((3, 4),), seed=0,
                         word_dimentions=40, activations=('iden',), dropout=0.2,
                         l1_regs=(0.0001, 0.0003, 0.00003, 0.0001),
                         update_finction=adam)
1cnn - 3:
Epoch 15 finished. Training time: 482.64 secs
Train score = 0.814012. Valid score = 0.815367.
Test score = 0.809995
OPTIMIZATION COMPLETE.
Best valid score: 0.823394
Best iter num: 45620, best epoch: 14
Last test score = 0.811093
Saving state to './cnn_states/state_2016-06-20-12:35:33'...
--- 6383.1793108 seconds ---

1cnn
    test_on_binary_sentiment(data_path='./data/binarySentiment/', clf_name='1cnn',
                             n_epochs=500, batch_size=50, non_static=True, early_stop=False, valid_frequency=50,
                             k_top=1, n_filters=(100,), windows=((3, 4),), seed=0,
                             word_dimentions=30, activations=('relu',), dropout=0.2,
                             l1_regs=(0.00001, 0.00003, 0.000003, 0.0001),
                             update_finction=adam)
Last test score = 0.785832.

==================================
amazon full:
[CV] .......... clf__C=1.0, clf__solver=lbfgs, score=0.738417 -42.4min
[CV] ...... clf__C=1.0, clf__solver=newton-cg, score=0.739802 -52.6min
[CV] ...... clf__C=1.0, clf__solver=newton-cg, score=0.734168 -56.0min



==========amazon_1000000===========
dataset name: /home/igoltsman/hdfs/preprocessed_data/amazon_1000000
classifier name: LogisticRegression
best params:
{'clf__C': 1.8, 'clf__solver': 'lbfgs'}
best score = 0.743014
--- 23715.153753 seconds --

dataset name: /home/igoltsman/hdfs/preprocessed_data/amazon_1000000
classifier name: MultinomialNB
best params:
{'clf__fit_prior': True, 'clf__alpha': 0.1}
best score = 0.671529
--- 3059.67851782 seconds ---

dataset name: /home/igoltsman/hdfs/preprocessed_data/amazon_1000000
classifier name: SGDClassifier
best params:
{'clf__penalty': 'none', 'clf__loss': 'hinge', 'clf__alpha': 4e-05}
best score = 0.735709


1cnn:
train_and_save_model(clf_name='1cnn', data_file=dt.get_output_name(dataset_name, model_name, max_size),
                     n_epochs=15, batch_size=50, non_static=True, early_stop=True, valid_frequency=20,
                     k_top=1, n_filters=(100,), windows=((3, 4),), seed=0, update_finction=adam,
                     word_dimentions=40, activations=('relu',), dropout=0.2,
                     l1_regs=(0.00001, 0.00001, 0.00001, 0.0001, 0.0001), n_hidden=100,
                     l2_regs=(0.0001 / 2, 0.00003 / 2, 0.000003 / 2, 0.0001 / 2),
                     big_dataset=True)

global_iter 7400, epoch 1, batch 7400, mean train cost = 0.799833
------------valid score: 0.737870------------
global_iter 7540, epoch 1, batch 7540, mean train cost = 0.751871
------------valid score: 0.738320------------
global_iter 8000, epoch 1, batch 8000, mean train cost = 0.767206
------------valid score: 0.739410------------
global_iter 8960, epoch 1, batch 8960, mean train cost = 0.758846
------------valid score: 0.741110------------
global_iter 9000, epoch 1, batch 9000, mean train cost = 0.783307
------------valid score: 0.741520------------
global_iter 9060, epoch 1, batch 9060, mean train cost = 0.782503
------------valid score: 0.741740------------
global_iter 9080, epoch 1, batch 9080, mean train cost = 0.799477
------------valid score: 0.742440------------
global_iter 9820, epoch 1, batch 9820, mean train cost = 0.799475
------------valid score: 0.743480------------
global_iter 10080, epoch 1, batch 10080, mean train cost = 0.805060
------------valid score: 0.743760------------
global_iter 10100, epoch 1, batch 10100, mean train cost = 0.796102
------------valid score: 0.743940------------
global_iter 10400, epoch 1, batch 10400, mean train cost = 0.760074
------------valid score: 0.744390------------
global_iter 11380, epoch 1, batch 11380, mean train cost = 0.771251
------------valid score: 0.745700------------
global_iter 11700, epoch 1, batch 11700, mean train cost = 0.798603
------------valid score: 0.746040------------
global_iter 12500, epoch 1, batch 12500, mean train cost = 0.782860
------------valid score: 0.747140------------
global_iter 12940, epoch 1, batch 12940, mean train cost = 0.825887
------------valid score: 0.747410------------

============imdb===============
('Train ', 0.017642105263157903, 'Valid ', 0.095999999999999974, 'Test ', 0.16000000000000003)
===============================
hadoop:
model options {'encoder': 'lstm', 'optimizer': <function adadelta at 0x7fe613d1a1b8>, 'validFreq': 700,
'lrate': 0.0001, 'batch_size': 16, 'decay_c': 0.0, 'patience': 10, 'reload_model': None, 'n_words': 100000,
'max_epochs': 100, 'dispFreq': 10, 'dataset': 'amazon', 'valid_batch_size': 64, 'use_dropout': True,
'dim_proj': 128, 'maxlen': 182, 'saveto': None, 'noise_std': 0.0, 'test_size': -1, 'saveFreq': 10}

this laptop:
model options {'encoder': 'lstm', 'optimizer': <function adadelta at 0x7fcc574ba1b8>, 'validFreq': 700,
'lrate': 0.0001, 'batch_size': 16, 'decay_c': 0.0, 'patience': 10, 'reload_model': None, 'n_words': 50000,
'max_epochs': 100, 'dispFreq': 10, 'dataset': 'twitter', 'valid_batch_size': 64, 'use_dropout': True,
'dim_proj': 128, 'maxlen': None, 'saveto': None, 'noise_std': 0.0, 'test_size': -1, 'saveFreq': 10}

AWS:
model options {'encoder': 'lstm', 'optimizer': <function adadelta at 0x7faee26e0230>, 'validFreq': 700,
'lrate': 0.0001, 'batch_size': 16, 'decay_c': 0.0, 'patience': 10, 'reload_model': None, 'n_words': 100000,
'max_epochs': 100, 'dispFreq': 10, 'dataset': '20_news', 'valid_batch_size': 64, 'use_dropout': True,
'dim_proj': 128, 'maxlen': None, 'saveto': None, 'noise_std': 0.0, 'test_size': 628, 'saveFreq': 10}
